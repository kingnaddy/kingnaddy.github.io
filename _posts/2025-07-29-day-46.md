---
layout: post
title: "Day 46 – Heavy ML Failures, Threshold Tweaks & Ensemble Exploration"
date: 2025-07-29
author: Noble Adike
permalink: /day46.html
tags: ["Machine Learning", "Threshold Tuning", "Class Imbalance", "Binary Classification", "SMOTE", "RandomForest", "Ensemble Models"]

what_i_learned: |
  Today was an intense deep-dive into model troubleshooting and pipeline reengineering. I started by diagnosing our multiclass performance issues and confirming that the majority class was suppressing all others, especially in recall. To address this, I implemented class-weighted loss and SMOTE oversampling, then followed up with fine-grained threshold tuning guided by PR curves. I also aligned evaluation metrics directly with our operational goals, defining clear recall and precision targets. Despite all this, our single-model pipelines still couldn't meet every threshold, which led me to explore two-stage architectures and ensemble methods. Although I didn’t land on a perfect solution today, I gained a better understanding of the constraints and next steps.

blockers: |
   Could not find a probability threshold that satisfied all business metric targets for both urgent and non-urgent classes. Ensemble models like EasyEnsembleClassifier showed promise but required more tuning and time than we had today. Even binary classifiers struggled under extreme imbalance, revealing the need for better data or augmentation.

reflection: |
  Today was tough and possibly the most failure-heavy day I’ve had on this project, but it was also incredibly revealing. I realized how hard it is to force precision and recall into compliance without good data balance or additional signal strength. It’s clear now that clever thresholding alone can’t save a flawed distribution. The decision to pivot to binary classification and eventually synthetic data for rapid iteration was difficult but necessary. I also learned that some challenges require not just code, but strategy—thinking in architectures rather than models. Tomorrow I aim to finalize the results section with honesty and clarity, highlighting both our wins and the real obstacles we faced.
---
